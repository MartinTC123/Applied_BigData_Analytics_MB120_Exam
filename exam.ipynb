{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models import ML_Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import inspect\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2012-01-01\"\n",
    "end_date = dt.date.today()\n",
    "main_col = \"Adj Close\"\n",
    "interval = \"1d\"\n",
    "stocks_list = [\"EQNR.OL\", \"DNB.OL\", \"TEL.OL\", \"NHY.OL\"]#, \"AKRBP.OL\", \"YAR.OL\", \"MOWI.OL\", \"CL=F\", \"OSEBX.OL\"]\n",
    "indicators = [\"MA5\", \"MA20\", \"MA50\", \"MA200\"]#, \"MIN\", \"MAX\", \"LOG_RET\", \"MOM\", \"VOLA\", \"DIFF\"]\n",
    "models = [\"Linear\", \"DTR\", \"MLP\"]\n",
    "metric_names = [\"r2\", \"neg_mean_absolute_error\", \"neg_root_mean_squared_error\", \"neg_mean_absolute_percentage_error\"]\n",
    "pretty_metric_names = {\n",
    "                \"r2\":\"R^2 Error: \", \n",
    "                \"neg_mean_absolute_error\":\"Mean Absolute Error: \", \n",
    "                \"neg_root_mean_squared_error\":\"Root Mean Squared Error: \", \n",
    "                \"neg_mean_absolute_percentage_error\":\"Mean Absolute Percentage Error: \"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_models = ML_Models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting data from Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = {}\n",
    "for ticker in stocks_list:\n",
    "    print(f\"Downloading {ticker} data\")\n",
    "    # fetch stock data from yahoo finance\n",
    "    raw_data = yf.download(ticker, start=start_date, end=end_date, interval=interval)\n",
    "    stock_data[ticker] = raw_data\n",
    "\n",
    "# Save fetched data to csv\n",
    "#for ticker in stocks_list:\n",
    "#    stock_data[ticker].to_csv(\"Saved_Data/rawdata_\"+ticker+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccesing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indicator_columns(data, indicators):\n",
    "        # Creating label and shifting the selected main_feature value by 1.\n",
    "        label_name = \"Label\"\n",
    "        label_name = label_name\n",
    "        data[label_name] = data[main_col].shift(periods=1)\n",
    "\n",
    "        # Checking which of the different indicators that should be added as a column (based on input from indicators list).\n",
    "        if \"MA5\" in indicators:\n",
    "            data[\"MA5\"] = data[label_name].rolling(5).mean()\n",
    "        if \"MA20\" in indicators:\n",
    "            data[\"MA20\"] = data[label_name].rolling(20).mean()\n",
    "        if \"MA50\" in indicators:\n",
    "            data[\"MA50\"] = data[label_name].rolling(50).mean()\n",
    "        if \"MA200\" in indicators:\n",
    "            data[\"MA200\"] = data[label_name].rolling(200).mean()\n",
    "        if \"MIN\" in indicators:\n",
    "            data[\"MIN\"] = data[label_name].rolling(20).min()\n",
    "        if \"MAX\" in indicators:\n",
    "            data[\"MAX\"] = data[label_name].rolling(20).max()\n",
    "        log_ret = np.log(data[label_name] / data[label_name].shift(1))\n",
    "        if \"LOG_RET\" in indicators:\n",
    "            data[\"LOG_RET\"] = log_ret\n",
    "        if \"MOM\" in indicators:\n",
    "            data[\"MOM\"] = log_ret.rolling(20).mean()\n",
    "        if \"VOLA\" in indicators:\n",
    "            data[\"VOLA\"] = log_ret.rolling(20).std()\n",
    "        if \"DIFF\" in indicators:\n",
    "            data[\"DIFF\"] = data[label_name] - data[label_name].shift(1)\n",
    "\n",
    "        # remove empty vals.\n",
    "        data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_y_arrays(data, label_name):\n",
    "        # array that contains the indicators data.\n",
    "        X = data.loc[:, indicators].to_numpy()\n",
    "        # array with the target data (based on main_feature).\n",
    "        y = data[label_name].to_numpy()\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "def create_X_y_train_test_split(X, y, current_stock):\n",
    "        data = stock_data[current_stock]\n",
    "\n",
    "        for train_index, test_index in tscv.split(data):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the preprocessing\n",
    "for ticker, data in stock_data.items():\n",
    "    add_indicator_columns(data=data, indicators=indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that choose model from models class\n",
    "def pick_model(model):\n",
    "    if model == \"Linear\":\n",
    "        return exam_models.model_Linear()\n",
    "    elif model == \"DTR\":\n",
    "        return exam_models.model_DTR()\n",
    "    elif model == \"MLP\":\n",
    "        return exam_models.model_MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = {}\n",
    "trained_models = {}\n",
    "\n",
    "def train_models(input_models):\n",
    "        for ticker, data in stock_data.items():\n",
    "\n",
    "            X, y = create_X_y_arrays(data=data, label_name=\"Label\")\n",
    "\n",
    "            X_train, X_test, y_train, y_test = create_X_y_train_test_split(X=X, y=y, current_stock=ticker)\n",
    "\n",
    "            # Evaluating and training selected models.\n",
    "            for model_i in input_models:\n",
    "                model = pick_model(model=model_i)\n",
    "                metric_dict = {}\n",
    "                for metric_name in metric_names:\n",
    "                    metric_dict[metric_name] = metric_name\n",
    "                    # using method from sci-kit lib to cross-validate\n",
    "                    cross_val_results = cross_validate(\n",
    "                        model,\n",
    "                        X,\n",
    "                        y,\n",
    "                        cv=tscv,\n",
    "                        scoring=metric_dict,\n",
    "                        return_train_score=True,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=0  \n",
    "                    )\n",
    "                    model.fit(X_train, y_train)\n",
    "                cv_results[ticker+\"_model_\"+model_i] = cross_val_results \n",
    "                trained_models[\"trained_model_\"+model_i+\"_\"+ticker] = model\n",
    "\n",
    "        return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_stocks_models = train_models(input_models=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trained_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting values based on trained models &\n",
    "# Evaluating them based on error metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_predictions = {}\n",
    "y_pred = 0\n",
    "def predict_trained_models(input_models):\n",
    "    for ticker, data in stock_data.items():\n",
    "\n",
    "        # Creating X and y arrays for train and test sets.\n",
    "        X, y = create_X_y_arrays(data=data, label_name= \"Label\")\n",
    "\n",
    "        X_train, X_test, y_train, y_test = create_X_y_train_test_split(X=X, y=y, current_stock=ticker)\n",
    "\n",
    "        last_train_index, last_test_index = None, None\n",
    "\n",
    "        for train_index, test_index in tscv.split(data):\n",
    "            last_train_index, last_test_index = train_index, test_index\n",
    "\n",
    "        prediction = data.loc[data.index[last_test_index], [main_col]].copy(deep=True)\n",
    "        stock_predictions[ticker] = prediction\n",
    "\n",
    "        for model_i in input_models:\n",
    "            model = trained_models[\"trained_model_\"+model_i+\"_\"+ticker]\n",
    "            y_pred = model.predict(X_test)\n",
    "            prediction.loc[:, model_i+\" Prediction\"] = y_pred\n",
    "\n",
    "    return stock_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_data = predict_trained_models(input_models=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in stocks_list:\n",
    "    print(ticker)\n",
    "    display(predicted_stock_data[ticker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(save_data_to_df=True):\n",
    "    for ticker, data in stock_data.items():\n",
    "        print(\"\\n--------\", \"Stock:\", ticker, \"--------\")\n",
    "\n",
    "        # prepare dataframe if requested\n",
    "        metrics_df_container = {}\n",
    "        if save_data_to_df:\n",
    "            metrics_df = pd.DataFrame(index=list(pretty_metric_names.values()))\n",
    "            metrics_df_container[ticker] = metrics_df\n",
    "            current_metrics_df = metrics_df\n",
    "\n",
    "        for model_name in models:        \n",
    "            cv = cv_results[ticker+\"_model_\"+model_name]\n",
    "\n",
    "            print(\"---\", \"Model:\", model_name, \"---\")\n",
    "            print(\"--\", \"Training Scores:\", \"--\")\n",
    "            split1_errors_string = f\"Score for first data split -\\n\"\n",
    "            split3_errors_string = f\"Score for third data split -\\n\"\n",
    "            split5_errors_string = f\"Score for last data split -\\n\"\n",
    "\n",
    "            for error_metric_name in metric_names:\n",
    "                # some metrics are saved as negative, so change sign\n",
    "                if error_metric_name.startswith(\"neg\"):\n",
    "                    try:\n",
    "                        error_metric_value = -cv[\"train\"+\"_\"+error_metric_name]\n",
    "                    # string being passed that can't be negative\n",
    "                    except:\n",
    "                        error_metric_value = cv[\"train\"+\"_\"+error_metric_name]\n",
    "                else:\n",
    "                    error_metric_value = cv[\"train\"+\"_\"+error_metric_name]\n",
    "\n",
    "                if save_data_to_df:\n",
    "                    # pass string, used to indicate missing metrics\n",
    "                    if isinstance(error_metric_value, str):\n",
    "                        current_metrics_df.loc[pretty_metric_names[error_metric_name], model_name+\" Model \"+\"Train\"] = error_metric_value\n",
    "                    # otherwise mean of all splits\n",
    "                    else:\n",
    "                        current_metrics_df.loc[pretty_metric_names[error_metric_name], model_name+\" Model \"+\"Train\"] = np.mean(error_metric_value)\n",
    "        \n",
    "                if isinstance(error_metric_value, str):\n",
    "                    split1_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value}\\n\"\n",
    "                else:\n",
    "                    split1_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value[0]:.3f}\\n\"\n",
    "\n",
    "                if isinstance(error_metric_value, str):\n",
    "                    split3_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value}\\n\"\n",
    "                else:\n",
    "                    split3_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value[2]:.3f}\\n\"\n",
    "    \n",
    "                if isinstance(error_metric_value, str):\n",
    "                    split5_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value}\\n\"\n",
    "                else:\n",
    "                    split5_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value[-1]:.3f}\\n\"\n",
    "\n",
    "            # print the two strings\n",
    "            print(split1_errors_string+\"\\n\"+split3_errors_string+\"\\n\"+split5_errors_string)\n",
    "            \n",
    "            # print a newline to separate from other prints\n",
    "            print(\"\", end=\"\\n\")\n",
    "\n",
    "            print(\"--\", \"Testing Scores:\", \"--\")\n",
    "            split1_errors_string = f\"Score for first data split -\\n\"\n",
    "            split3_errors_string = f\"Score for third data split -\\n\"\n",
    "            split5_errors_string = f\"- Score for last data split -\\n\"\n",
    "\n",
    "            for error_metric_name in metric_names:\n",
    "                # some metrics are saved as negative, so change sign\n",
    "                if error_metric_name.startswith(\"neg\"):\n",
    "                    try:\n",
    "                        error_metric_value = -cv[\"test\"+\"_\"+error_metric_name]\n",
    "                    # string being passed that can't be negative\n",
    "                    except:\n",
    "                        error_metric_value = cv[\"test\"+\"_\"+error_metric_name]\n",
    "                else:\n",
    "                    error_metric_value = cv[\"test\"+\"_\"+error_metric_name]\n",
    "\n",
    "                if save_data_to_df:\n",
    "                    # pass string, used to indicate missing metrics\n",
    "                    if isinstance(error_metric_value, str):\n",
    "                        current_metrics_df.loc[pretty_metric_names[error_metric_name], model_name+\" Model \"+\"Test\"] = error_metric_value\n",
    "                    # otherwise mean of all splits\n",
    "                    else:\n",
    "                        current_metrics_df.loc[pretty_metric_names[error_metric_name], model_name+\" Model \"+\"Test\"] = np.mean(error_metric_value)\n",
    "        \n",
    "                if isinstance(error_metric_value, str):\n",
    "                    split1_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value}\\n\"\n",
    "                else:\n",
    "                    split1_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value[0]:.3f}\\n\"\n",
    "\n",
    "                if isinstance(error_metric_value, str):\n",
    "                    split3_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value}\\n\"\n",
    "                else:\n",
    "                    split3_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value[2]:.3f}\\n\"\n",
    "\n",
    "                if isinstance(error_metric_value, str):\n",
    "                    split5_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value}\\n\"\n",
    "                else:\n",
    "                    split5_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value[-1]:.3f}\\n\"\n",
    "\n",
    "            # print the two strings\n",
    "            print(split1_errors_string+\"\\n\"+split3_errors_string+\"\\n\"+split5_errors_string)\n",
    "\n",
    "            # print a newline to separate from other prints\n",
    "            print(\"\", end=\"\\n\")\n",
    "\n",
    "    if save_data_to_df:\n",
    "        return metrics_df\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_output = print_metrics(save_data_to_df=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for insights about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT FOR SHOWING DIFFERENT DATA SPLITS FOR THE DIFFERENT STOCKS\n",
    "for ticker in stocks_list:\n",
    "    figure, axs = plt.subplots(5, figsize=(16,20))\n",
    "    figure.set_tight_layout(True)\n",
    "\n",
    "    data = stock_data[ticker]\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    current_splits = tscv.split(data)\n",
    "        \n",
    "    i = 0\n",
    "    for axes, (train_index, test_index) in zip(axs, current_splits):\n",
    "        axes.plot(data.index[train_index], data.loc[data.index[train_index], main_col], label=f\"Training data {i+1}\", color=\"blue\")\n",
    "        axes.plot(data.index[test_index], data.loc[data.index[test_index], main_col], label=f\"Test data {i+1}\", color=\"red\")\n",
    "\n",
    "        axes.set_xlim(data.index[0], data.index[-1])\n",
    "\n",
    "        axes.set_title(f\"Train / test split {i+1} for {ticker}\")\n",
    "        axes.set_xlabel(\"Date\")\n",
    "        axes.set_ylabel(f\"{main_col}\")\n",
    "        axes.legend()\n",
    "\n",
    "        i = i+1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT FOR COMPARING ACTUAL TO PREDICTED VALUES\n",
    "for ticker, data in stock_data.items():\n",
    "    figure, axs = plt.subplots(figsize=(32,16))\n",
    "\n",
    "    X, y = create_X_y_arrays(data=data, label_name=\"Label\")\n",
    "    X_train, X_test, y_train, y_test = create_X_y_train_test_split(X=X, y=y, current_stock=ticker)\n",
    "\n",
    "    X_test_index = np.arange(X_train.shape[0], X_train.shape[0]+X_test.shape[0])\n",
    "\n",
    "\n",
    "    plt.plot(data.index[X_test_index], y_test, color='blue', label='Actual', linewidth=\"4.0\")\n",
    "    plt.plot(data.index[X_test_index], stock_predictions[ticker][main_col], color='red', label='Predicted')\n",
    "    plt.title(f'Actual vs Predicted, {ticker}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
