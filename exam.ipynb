{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models import ML_Models\n",
    "exam_models = ML_Models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import inspect\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters used for fetching the data from yfinance.\n",
    "start_date = \"2012-01-01\"\n",
    "end_date = dt.date.today()\n",
    "main_col = \"Adj Close\"\n",
    "interval = \"1d\"\n",
    "stocks_list = [\"EQNR.OL\", \"DNB.OL\", \"TEL.OL\", \"NHY.OL\", \"AKRBP.OL\", \"YAR.OL\", \"MOWI.OL\", \"CL=F\", \"OSEBX.OL\"]\n",
    "\n",
    "# Specifying the indicators wanted for further analysis.\n",
    "indicators = [\"MA5\", \"MA20\", \"MA50\", \"MA200\", \"MIN\", \"MAX\", \"LOG_RET\", \"MOM\", \"VOLA\", \"DIFF\"]\n",
    "\n",
    "# Models to utilize for forecasting/prediction. \n",
    "# Name of the models are based on the pick_model function in Models.py\n",
    "models = [\"LR\", \"DTR\", \"MLP\", \"XGBoost\", \"XGBoost_LR\", \"ADA\", \"ADA_LR\", \"ADA_MLP\" \"GBR\", \"Bagging\", \"Bagging_LR\",\"StackedRegressor\"]\n",
    "\n",
    "# Metrics used to evaluate the performance of each model.\n",
    "# MAE, MSE, RMSE and MAPE are named with \"neg_\" to be recognized by the cross_validate function from scikit-learn.\n",
    "metric_names = [\"r2\", \"neg_mean_absolute_error\", \"neg_mean_squared_error\", \"neg_root_mean_squared_error\", \"neg_mean_absolute_percentage_error\"]\n",
    "pretty_metric_names = {\"r2\":\"R^2: \", \"neg_mean_absolute_error\":\"MAE: \", \"neg_mean_squared_error\":\"MSE: \",\"neg_root_mean_squared_error\":\"RMSE: \", \"neg_mean_absolute_percentage_error\":\"MAPE: \"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting data from Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading EQNR.OL data\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading DNB.OL data\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading TEL.OL data\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading NHY.OL data\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading AKRBP.OL data\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading YAR.OL data\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading MOWI.OL data\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading CL=F data\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading OSEBX.OL data\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "All the data is now downloaded!\n"
     ]
    }
   ],
   "source": [
    "stock_data = {}\n",
    "for ticker in stocks_list:\n",
    "    print(f\"Downloading {ticker} data\")\n",
    "    # fetch stock data from yahoo finance\n",
    "    raw_data = yf.download(ticker, start=start_date, end=end_date, interval=interval)\n",
    "    stock_data[ticker] = raw_data\n",
    "\n",
    "print(\"All the data is now downloaded!\")\n",
    "\n",
    "# Save fetched data to csv\n",
    "#for ticker in stocks_list:\n",
    "    #stock_data[ticker].to_csv(\"raw_data/data_\"+ticker+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccesing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indicator_columns(data, indicators):\n",
    "    # Creating label and shifting the selected main_col value by 1.\n",
    "    label_name = \"Label\"\n",
    "    data[label_name] = data[main_col].shift(periods=1)\n",
    "\n",
    "    # Checking which of the different indicators that should be added as a column (based on input from indicators list).\n",
    "    if \"MA5\" in indicators:\n",
    "        data[\"MA5\"] = data[label_name].rolling(5).mean()\n",
    "    if \"MA20\" in indicators:\n",
    "        data[\"MA20\"] = data[label_name].rolling(20).mean()\n",
    "    if \"MA50\" in indicators:\n",
    "        data[\"MA50\"] = data[label_name].rolling(50).mean()\n",
    "    if \"MA200\" in indicators:\n",
    "        data[\"MA200\"] = data[label_name].rolling(200).mean()\n",
    "    if \"MIN\" in indicators:\n",
    "        data[\"MIN\"] = data[label_name].rolling(20).min()\n",
    "    if \"MAX\" in indicators:\n",
    "        data[\"MAX\"] = data[label_name].rolling(20).max()\n",
    "    log_ret = np.log(data[label_name] / data[label_name].shift(1))\n",
    "    if \"LOG_RET\" in indicators:\n",
    "        data[\"LOG_RET\"] = log_ret\n",
    "    if \"MOM\" in indicators:\n",
    "        data[\"MOM\"] = log_ret.rolling(20).mean()\n",
    "    if \"VOLA\" in indicators:\n",
    "        data[\"VOLA\"] = log_ret.rolling(20).std()\n",
    "    if \"DIFF\" in indicators:\n",
    "        data[\"DIFF\"] = data[label_name] - data[label_name].shift(1)\n",
    "\n",
    "    # remove empty vals.\n",
    "    data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_y_arrays(data, label_name):\n",
    "        # array that contains the indicators data.\n",
    "        X = data.loc[:, indicators].to_numpy()\n",
    "        # array with the target data (based on main_col).\n",
    "        y = data[label_name].to_numpy()\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "def create_X_y_train_test_split(X, y, current_stock):\n",
    "    data = stock_data[current_stock]\n",
    "\n",
    "    for train_index, test_index in tscv.split(data):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Adding the specified indicators to the data.\n",
    "for ticker, data in stock_data.items():\n",
    "    add_indicator_columns(data=data, indicators=indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = {}\n",
    "trained_models = {}\n",
    "\n",
    "def train_models(input_models):\n",
    "        for ticker, data in stock_data.items():\n",
    "\n",
    "            X, y = create_X_y_arrays(data=data, label_name=\"Label\")\n",
    "\n",
    "            X_train, X_test, y_train, y_test = create_X_y_train_test_split(X=X, y=y, current_stock=ticker)\n",
    "\n",
    "            # Evaluating and training selected models.\n",
    "            for model_i in input_models:\n",
    "                model = exam_models.pick_model(model=model_i)\n",
    "                metric_dict = {}\n",
    "                for metric_name in metric_names:\n",
    "                    metric_dict[metric_name] = metric_name\n",
    "\n",
    "                print(model)\n",
    "                    \n",
    "                # using method from sci-kit lib to cross-validate\n",
    "                cross_val_results = cross_validate(\n",
    "                    model,\n",
    "                    X,\n",
    "                    y,\n",
    "                    cv=tscv,\n",
    "                    scoring=metric_dict,\n",
    "                    return_train_score=True,\n",
    "                    n_jobs=-1,\n",
    "                    verbose=0  \n",
    "                )\n",
    "                model.fit(X_train, y_train)\n",
    "                cv_results[ticker+\"_model_\"+model_i] = cross_val_results \n",
    "                trained_models[\"trained_model_\"+model_i+\"_\"+ticker] = model\n",
    "\n",
    "        return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression()\n",
      "DecisionTreeRegressor()\n",
      "MLPRegressor()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "XGBRegressor(base_score=None, booster='gblinear', callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "AdaBoostRegressor()\n",
      "AdaBoostRegressor(estimator=LinearRegression())\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "estimator should be an estimator implementing 'fit' method, None was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cv_stocks_models \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m, in \u001b[0;36mtrain_models\u001b[0;34m(input_models)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# using method from sci-kit lib to cross-validate\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m cross_val_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtscv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m  \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     32\u001b[0m cv_results[ticker\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_model_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mmodel_i] \u001b[38;5;241m=\u001b[39m cross_val_results \n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:261\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    259\u001b[0m     scorers \u001b[39m=\u001b[39m check_scoring(estimator, scoring)\n\u001b[1;32m    260\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     scorers \u001b[39m=\u001b[39m _check_multimetric_scoring(estimator, scoring)\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:595\u001b[0m, in \u001b[0;36m_check_multimetric_scoring\u001b[0;34m(estimator, scoring)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(keys) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    594\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn empty dict was passed. \u001b[39m\u001b[39m{\u001b[39;00mscoring\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 595\u001b[0m     scorers \u001b[39m=\u001b[39m {\n\u001b[1;32m    596\u001b[0m         key: check_scoring(estimator, scoring\u001b[39m=\u001b[39mscorer)\n\u001b[1;32m    597\u001b[0m         \u001b[39mfor\u001b[39;00m key, scorer \u001b[39min\u001b[39;00m scoring\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    598\u001b[0m     }\n\u001b[1;32m    599\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err_msg_generic)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:596\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(keys) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    594\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn empty dict was passed. \u001b[39m\u001b[39m{\u001b[39;00mscoring\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    595\u001b[0m     scorers \u001b[39m=\u001b[39m {\n\u001b[0;32m--> 596\u001b[0m         key: check_scoring(estimator, scoring\u001b[39m=\u001b[39;49mscorer)\n\u001b[1;32m    597\u001b[0m         \u001b[39mfor\u001b[39;00m key, scorer \u001b[39min\u001b[39;00m scoring\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    598\u001b[0m     }\n\u001b[1;32m    599\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err_msg_generic)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:474\u001b[0m, in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Determine scorer from user options.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \n\u001b[1;32m    450\u001b[0m \u001b[39mA TypeError will be thrown if the estimator cannot be scored.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39m    ``scorer(estimator, X, y)``.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 474\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    475\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mestimator should be an estimator implementing \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m'\u001b[39m\u001b[39m method, \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m was passed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    476\u001b[0m         \u001b[39m%\u001b[39m estimator\n\u001b[1;32m    477\u001b[0m     )\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(scoring, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m get_scorer(scoring)\n",
      "\u001b[0;31mTypeError\u001b[0m: estimator should be an estimator implementing 'fit' method, None was passed"
     ]
    }
   ],
   "source": [
    "cv_stocks_models = train_models(input_models=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting values based on trained models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_predictions = {}\n",
    "def predict_trained_models(input_models):\n",
    "    for ticker, data in stock_data.items():\n",
    "\n",
    "        # Creating X and y arrays for train and test sets.\n",
    "        X, y = create_X_y_arrays(data=data, label_name= \"Label\")\n",
    "\n",
    "        X_train, X_test, y_train, y_test = create_X_y_train_test_split(X=X, y=y, current_stock=ticker)\n",
    "\n",
    "        last_train_index, last_test_index = None, None\n",
    "\n",
    "        for train_index, test_index in tscv.split(data):\n",
    "            last_train_index, last_test_index = train_index, test_index\n",
    "\n",
    "        prediction = data.loc[data.index[last_test_index], [main_col]].copy(deep=True)\n",
    "        stock_predictions[ticker] = prediction\n",
    "\n",
    "        for model_i in input_models:\n",
    "            model = trained_models[\"trained_model_\"+model_i+\"_\"+ticker]\n",
    "            y_pred = model.predict(X_test)\n",
    "            prediction.loc[:, model_i+\" Prediction\"] = y_pred\n",
    "\n",
    "    return stock_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_data = predict_trained_models(input_models=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in stocks_list:\n",
    "    print(ticker)\n",
    "    display(predicted_stock_data[ticker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_output = {}\n",
    "def print_metrics(save_data_to_df=True):\n",
    "    for ticker, data in stock_data.items():\n",
    "        print(\"\\n--------\", ticker, \"--------\")\n",
    "\n",
    "        # prepare dataframe if requested\n",
    "        if save_data_to_df:\n",
    "            metrics_df = pd.DataFrame(index=list(pretty_metric_names.values()))\n",
    "            metrics_df_output[ticker] = metrics_df\n",
    "            current_metrics_df = metrics_df\n",
    "\n",
    "        for model_name in models:        \n",
    "            cv = cv_results[ticker+\"_model_\"+model_name]\n",
    "\n",
    "            print(\"-\", model_name, \"-\")\n",
    "            print(\"-\", \"Training Scores:\", \"-\")\n",
    "            split1_errors_string = f\"Score for first data split \\n\"\n",
    "            split3_errors_string = f\"Score for third data split \\n\"\n",
    "            split5_errors_string = f\"Score for last data split \\n\"\n",
    "\n",
    "            for error_metric_name in metric_names:\n",
    "                # some metrics are saved as negative, so change sign\n",
    "                if error_metric_name.startswith(\"neg\"):\n",
    "                    try:\n",
    "                        error_metric_value = -cv[\"train\"+\"_\"+error_metric_name]\n",
    "                    # string being passed that can't be negative\n",
    "                    except:\n",
    "                        error_metric_value = cv[\"train\"+\"_\"+error_metric_name]\n",
    "                else:\n",
    "                    error_metric_value = cv[\"train\"+\"_\"+error_metric_name]\n",
    "\n",
    "                if save_data_to_df:\n",
    "                    # pass string, used to indicate missing metrics\n",
    "                    if isinstance(error_metric_value, str):\n",
    "                        current_metrics_df.loc[pretty_metric_names[error_metric_name], model_name+\" Model \"+\"Train\"] = error_metric_value\n",
    "                    # otherwise mean of all splits\n",
    "                    else:\n",
    "                        current_metrics_df.loc[pretty_metric_names[error_metric_name], model_name+\" Model \"+\"Train\"] = np.mean(error_metric_value)\n",
    "        \n",
    "                if isinstance(error_metric_value, str):\n",
    "                    split1_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value}\\n\"\n",
    "                else:\n",
    "                    split1_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value[0]:.3f}\\n\"\n",
    "\n",
    "                if isinstance(error_metric_value, str):\n",
    "                    split3_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value}\\n\"\n",
    "                else:\n",
    "                    split3_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value[2]:.3f}\\n\"\n",
    "    \n",
    "                if isinstance(error_metric_value, str):\n",
    "                    split5_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value}\\n\"\n",
    "                else:\n",
    "                    split5_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value[-1]:.3f}\\n\"\n",
    "\n",
    "            # print the two strings\n",
    "            print(split1_errors_string+\"\\n\"+split3_errors_string+\"\\n\"+split5_errors_string+\"\\n\")\n",
    "            \n",
    "            print(\"-\", \"Testing Scores:\", \"-\")\n",
    "            split1_errors_string = f\"Score for first data split \\n\"\n",
    "            split3_errors_string = f\"Score for third data split \\n\"\n",
    "            split5_errors_string = f\"Score for last data split \\n\"\n",
    "\n",
    "            for error_metric_name in metric_names:\n",
    "                # some metrics are saved as negative, so change sign\n",
    "                if error_metric_name.startswith(\"neg\"):\n",
    "                    try:\n",
    "                        error_metric_value = -cv[\"test\"+\"_\"+error_metric_name]\n",
    "                    # string being passed that can't be negative\n",
    "                    except:\n",
    "                        error_metric_value = cv[\"test\"+\"_\"+error_metric_name]\n",
    "                else:\n",
    "                    error_metric_value = cv[\"test\"+\"_\"+error_metric_name]\n",
    "\n",
    "                if save_data_to_df:\n",
    "                    # pass string, used to indicate missing metrics\n",
    "                    if isinstance(error_metric_value, str):\n",
    "                        current_metrics_df.loc[pretty_metric_names[error_metric_name], model_name+\" Model \"+\"Test\"] = error_metric_value\n",
    "                    # otherwise mean of all splits\n",
    "                    else:\n",
    "                        current_metrics_df.loc[pretty_metric_names[error_metric_name], model_name+\" Model \"+\"Test\"] = np.mean(error_metric_value)\n",
    "        \n",
    "                if isinstance(error_metric_value, str):\n",
    "                    split1_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value}\\n\"\n",
    "                else:\n",
    "                    split1_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value[0]:.3f}\\n\"\n",
    "\n",
    "                if isinstance(error_metric_value, str):\n",
    "                    split3_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value}\\n\"\n",
    "                else:\n",
    "                    split3_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value[2]:.3f}\\n\"\n",
    "\n",
    "                if isinstance(error_metric_value, str):\n",
    "                    split5_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value}\\n\"\n",
    "                else:\n",
    "                    split5_errors_string += f\"{pretty_metric_names[error_metric_name]} {error_metric_value[-1]:.3f}\\n\"\n",
    "\n",
    "            # print the two strings\n",
    "            print(split1_errors_string+\"\\n\"+split3_errors_string+\"\\n\"+split5_errors_string+\"\\n\")\n",
    "\n",
    "    if save_data_to_df:\n",
    "        return metrics_df_output\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_output = print_metrics(save_data_to_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for insights about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT FOR SHOWING DIFFERENT DATA SPLITS FOR THE DIFFERENT STOCKS\n",
    "for ticker in stocks_list:\n",
    "    fig, sub_plots = plt.subplots(n_splits, figsize=(16,20))\n",
    "    fig.set_tight_layout(True)\n",
    "\n",
    "    data = stock_data[ticker]\n",
    "    idx = data.index\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    splits = list(tscv.split(data))\n",
    "        \n",
    "    current_split = 1\n",
    "    for i in range(len(sub_plots)):\n",
    "        train_index, test_index = splits[i]\n",
    "\n",
    "        sub_plots[i].plot(idx[train_index], data.loc[idx[train_index], main_col], label=f\"Training data {current_split}\", color=\"blue\")\n",
    "        sub_plots[i].plot(idx[test_index], data.loc[idx[test_index], main_col], label=f\"Test data {current_split}\", color=\"red\")\n",
    "        sub_plots[i].set_xlim(idx[0], idx[-1])\n",
    "        sub_plots[i].set_title(f\"Train / test split {current_split} for {ticker}\")\n",
    "        sub_plots[i].set_xlabel(\"Date\")\n",
    "        sub_plots[i].set_ylabel(f\"{main_col}\")\n",
    "        sub_plots[i].legend()\n",
    "\n",
    "        current_split = current_split + 1\n",
    "    \n",
    "    #fig.savefig(\"data_splits_plots/\"+ticker+\".png\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT FOR COMPARING ACTUAL TO THE DIFFERENT MODELS PREDICTED VALUES\n",
    "for ticker, data in stock_data.items():\n",
    "    figure, axs = plt.subplots(figsize=(32,16))\n",
    "    \n",
    "    X, y = create_X_y_arrays(data=data, label_name=\"Label\")\n",
    "    X_train, X_test, y_train, y_test = create_X_y_train_test_split(X=X, y=y, current_stock=ticker)\n",
    "\n",
    "    X_test_index = np.arange(X_train.shape[0], X_train.shape[0]+X_test.shape[0])\n",
    "\n",
    "    plt.plot(data.index[X_test_index], y_test, color='purple', label='Actual', linewidth=\"4.0\")\n",
    "    for model in models:\n",
    "        plt.plot(data.index[X_test_index], stock_predictions[ticker][model+\" Prediction\"], label=model)\n",
    "    plt.title(f'Actual vs Predicted, {ticker}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "\n",
    "    #plt.savefig(\"actual_predicted_plots/\"+ticker+\".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION PLOT BETWEEN THE DIFFERENT STOCKS\n",
    "adj_close_prices = pd.DataFrame({i: j[main_col] for i, j in stock_data.items()})\n",
    "\n",
    "corr = adj_close_prices.corr()\n",
    "\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", annot=True)\n",
    "\n",
    "plt.title(\"Correlation between stocks\")\n",
    "\n",
    "#plt.savefig(\"correlation_plots/correlation_stocks.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in stocks_list:\n",
    "    #metrics_output[ticker].to_csv(\"saved_metrics/stock_\"+ticker+\".csv\")\n",
    "    #stock_data[ticker].to_pickle(\"saved_data_pickle/stock_\"+ticker+\".pkl\")\n",
    "    #predicted_stock_data[ticker].to_csv(\"saved_predictions/predicted_data_\"+ticker+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, d in stock_data.items():\n",
    "    print(t)\n",
    "    print(d[\"VOLA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
